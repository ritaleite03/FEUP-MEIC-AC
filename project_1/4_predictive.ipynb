{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Model\n",
    "\n",
    "The metrics that will be used to evaluate this stage are:\n",
    "\n",
    "- **Diversity** of tasks (use of classification and regression) and of algorithms, this is, tested more than 4 with significantly different language bias OR with a significant number of variants.\n",
    "\n",
    "- **Parameter Tuning**, with a systematic approach.\n",
    "\n",
    "- **Understanding Algorithm Behavior**, solid (even if not deep) understanding of the behavior of most algorithms used OR <3 algorithms, also understanding the effect of parameters.\n",
    "\n",
    "- Training and testing on properly separated data, with multiple splits.\n",
    "\n",
    "- **Performance Estimation**, additional factors correctly taken into account (e.g. time), focus on performance measures aligned with DM goals and data characteristics, advanced performance measures (e.g. AUC), adequate baseline, correct analysis of values ​​for comparison, including tests of statistical significance, correct estimate of overfitting.\n",
    "\n",
    "- **Model Improvement**, development guided by performance improvement goals, even if pedagogical goals have not been ignored.\n",
    "\n",
    "- **Feature Importance**, correctly interpreted, related to the application domain.\n",
    "\n",
    "- Analysis of \"white box\" models, correctly interpreted, related to the application domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import optuna\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "BEST_MODELS = {}\n",
    "\n",
    "\n",
    "teams = pd.read_csv(\"data_prepared/teams.csv\")\n",
    "label_encoder = LabelEncoder()\n",
    "numerical_features = teams.select_dtypes(include=['float', 'int']).columns\n",
    "numerical_features = numerical_features.drop('year')\n",
    "scaler = StandardScaler()\n",
    "teams[numerical_features] = scaler.fit_transform(teams[numerical_features])\n",
    "teams['playoff'] = teams['playoff'].map({'Y': 1, 'N': 0})\n",
    "teams['confID'] = teams['confID'].map({'EA': 0, 'WE': 1})\n",
    "\n",
    "\n",
    "def encode_categorical_columns(df):\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        if col == 'playoff' or col == 'confID': continue\n",
    "        else: df[col] = label_encoder.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "\n",
    "encode_categorical_columns(teams)\n",
    "teams = teams.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para normalizar previsões\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "def normalize_predictions(predictions):\n",
    "    return (predictions - np.min(predictions)) / (np.max(predictions) - np.min(predictions))\n",
    "\n",
    "\n",
    "# Função para calcular o erro\n",
    "def get_error(pred_proba, label_playoff):\n",
    "    errors = []\n",
    "    for pred, label in zip(pred_proba, label_playoff):\n",
    "        errors.append(abs(pred - label))\n",
    "    return sum(errors)\n",
    "\n",
    "\n",
    "# Função para dividir os dados em treino e teste\n",
    "def get_train_and_test_data(data, year):\n",
    "    train = data[((data['year'] < year) | (data['year'] > year)) & (data['year'] != 11)].drop(\"year\", axis=1)\n",
    "    test = data[data['year'] == year].drop(\"year\", axis=1)\n",
    "    X_train, Y_train = train.drop(\"playoff\", axis=1), train[\"playoff\"]\n",
    "    X_test, Y_test = test.drop(\"playoff\", axis=1), test[\"playoff\"]\n",
    "    smote = SMOTE(random_state=SEED)\n",
    "    X_train, Y_train = smote.fit_resample(X_train, Y_train)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "# Select best features using RFECV\n",
    "def select_best_features(X_train, Y_train, feature_list, n_features):\n",
    "    selector = SelectKBest(f_classif, k=n_features)\n",
    "    selector.fit(X_train[feature_list], Y_train)\n",
    "    # Obter as features selecionadas\n",
    "    selected_features = [feature_list[i] for i in range(len(feature_list)) if selector.get_support()[i]]    \n",
    "    print(selected_features)\n",
    "    return selected_features + ['tmID']\n",
    "\n",
    "\n",
    "# Função para rodar o modelo\n",
    "def run_model(model, data, year, number, only_df=False, n_features=20):\n",
    "    feature_list = [\n",
    "        'o_fgm', 'o_fga', 'o_ftm', 'o_fta', 'o_3pm', 'o_3pa', 'o_oreb', 'o_dreb', 'o_asts',\n",
    "        'o_pf', 'o_stl', 'o_to', 'o_blk', 'o_pts', 'd_fgm', 'd_fga', 'd_ftm', 'd_fta', 'd_3pm',\n",
    "        'd_3pa', 'd_oreb', 'd_dreb', 'd_asts', 'd_pf', 'd_stl', 'd_to', 'd_blk', 'd_pts', 'won',\n",
    "        'lost', 'homeW', 'homeL', 'awayW', 'awayL', 'confW', 'confL', 'min', 'wonPost', 'lostPost',\n",
    "        'wonPointsPost', 'lostPointsPost', 'awards_players', 'awards_coaches', 'offensive_efficiency',\n",
    "        'defensive_efficiency', 'play_percent', 'factors4', 'possession', 'opponent_possession',\n",
    "        'avg_pie', 'avg_per'\n",
    "    ]\n",
    "        \n",
    "    X_train, Y_train, X_test, Y_test = get_train_and_test_data(data, year)\n",
    "    selected_features = select_best_features(X_train, Y_train, feature_list, n_features)\n",
    "    X_train, X_test = X_train[selected_features], X_test[selected_features]\n",
    "    \n",
    "    start_timer = time.time()\n",
    "    model.fit(X_train, Y_train)\n",
    "    # model = CalibratedClassifierCV(model, method='sigmoid', cv='prefit')\n",
    "    # model.fit(X_train, Y_train)\n",
    "\n",
    "    # Fazer as previsões\n",
    "    if isinstance(model, (LinearRegression, RandomForestRegressor)):\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = normalize_predictions(y_pred)\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    error = get_error(y_pred_proba, Y_test)\n",
    "\n",
    "    # Preparar as previsões binárias\n",
    "    number = min(number, len(y_pred_proba))\n",
    "\n",
    "    # Preparar as previsões binárias\n",
    "    y_pred = np.zeros_like(y_pred_proba)\n",
    "    top_indices = np.argsort(y_pred_proba)[-number:]\n",
    "    y_pred[top_indices] = 1\n",
    "    stop_timer = time.time()\n",
    "\n",
    "    # Construir o DataFrame de previsões\n",
    "    prediction_df = pd.DataFrame()\n",
    "    prediction_df['tmID'] = label_encoder.inverse_transform(X_test['tmID'])\n",
    "    prediction_df['Playoff'] = y_pred_proba\n",
    "    if only_df: return prediction_df\n",
    "    prediction_df['Playoff_Binary'] = y_pred\n",
    "    prediction_df['Playoff_Labeled'] = Y_test.values\n",
    "    \n",
    "    # Estatísticas\n",
    "    time_elapsed = stop_timer - start_timer\n",
    "    precision = precision_score(Y_test, y_pred)\n",
    "    recall = recall_score(Y_test, y_pred)\n",
    "    f1 = f1_score(Y_test, y_pred)\n",
    "    accuracy = accuracy_score(Y_test, y_pred)\n",
    "    auc = roc_auc_score(Y_test, y_pred)\n",
    "\n",
    "    return prediction_df, {\n",
    "        \"time\": time_elapsed,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"auc\": auc,\n",
    "        \"error\": error\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_accuracy = {}\n",
    "results_precision = {}\n",
    "results_error = {}\n",
    "\n",
    "def run_model_by_conferences(model, year):\n",
    "    \n",
    "    if year == 11:\n",
    "        east_pred_df = run_model(model, teams[teams['confID'] == 0], year, 4, True)       \n",
    "        weast_pred_df = run_model(model, teams[teams['confID'] == 1], year, 4, True)    \n",
    "        \n",
    "        east_sum = east_pred_df['Playoff'].sum()\n",
    "        weast_sum = weast_pred_df['Playoff'].sum()\n",
    "        \n",
    "       \n",
    "        pred_df = pd.concat([east_pred_df, weast_pred_df])\n",
    "        pred_df['tmID'] = pred_df['tmID'].replace(\"DET\", \"TUL\")\n",
    "        pred_df = pred_df.sort_values('tmID')\n",
    "        \n",
    "        # pred_sum = pred_df['Playoff'].sum()\n",
    "        # pred_df['Playoff'] = (pred_df['Playoff'] / pred_sum) * 8\n",
    "        # pred_df['Playoff'] = pred_df['Playoff'].clip(0, 1)\n",
    "        pred_df['Playoff'] = pred_df['Playoff'].round(2)\n",
    "        \n",
    "        print(pred_df['Playoff'].sum())\n",
    "        return pred_df\n",
    "\n",
    "    east_pred_df, east_statistics = run_model(model, teams[teams['confID'] == 0], year, 4)\n",
    "    weast_pred_df, weast_statistics = run_model(model, teams[teams['confID'] == 1], year, 4)\n",
    "\n",
    "    statistics = {}\n",
    "\n",
    "    for k in east_statistics.keys():\n",
    "        statistics[k] = (east_statistics[k] + weast_statistics[k]) / 2\n",
    "    \n",
    "    print(pd.concat([east_pred_df, weast_pred_df]))\n",
    "    print(f\"Time: {statistics['time']:.3f}    Error: {statistics['error']:.2f}    Accuracy: {statistics['accuracy']:.2f}    Precision: {statistics['precision']:.2f}    Recall: {statistics['recall']:.2f}    F1: {statistics['f1']:.2f}    AUC: {statistics['auc']:.2f}\")\n",
    "    print()\n",
    "    \n",
    "    results_accuracy[str(model.__class__.__name__)] = statistics['accuracy']\n",
    "    results_precision[str(model.__class__.__name__)] = statistics['precision']\n",
    "    results_error[str(model.__class__.__name__)] = statistics['error']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_objective(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 20)\n",
    "    min_samples_split = trial.suggest_float('min_samples_split', 0.1, 1.0)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "    max_features = trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])\n",
    "    criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "\n",
    "    scores = []\n",
    "    for i in range(2,11):\n",
    "        X_train, Y_train, _, _ = get_train_and_test_data(teams, i)\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            max_features=max_features,\n",
    "            criterion=criterion,\n",
    "            random_state=42\n",
    "        )\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        score = cross_val_score(model, X_train, Y_train, cv=cv, scoring='accuracy').mean()\n",
    "        scores.append(score)\n",
    "        \n",
    "    return 1 - np.mean(scores)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "study.optimize(decision_tree_objective, n_trials=50)\n",
    "\n",
    "\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "final_model = DecisionTreeClassifier(**study.best_params,random_state=42)\n",
    "run_model_by_conferences(final_model, 11)\n",
    "BEST_MODELS[\"decision_tree\"] = run_model_by_conferences(final_model, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_objective(trial):\n",
    "    C = trial.suggest_loguniform('C', 1e-5, 1e2)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "    gamma = trial.suggest_loguniform('gamma', 1e-5, 1e1) if kernel in ['rbf', 'poly', 'sigmoid'] else 'scale'\n",
    "    degree = trial.suggest_int('degree', 2, 5) if kernel == 'poly' else 3\n",
    "    coef0 = trial.suggest_float('coef0', -1, 1) if kernel in ['poly', 'sigmoid'] else 0\n",
    "    class_weight = trial.suggest_categorical('class_weight', [None, 'balanced'])\n",
    "\n",
    "    scores = []\n",
    "    for i in range(2,11):\n",
    "        X_train, Y_train, _, _ = get_train_and_test_data(teams, i)\n",
    "        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, coef0=coef0, class_weight=class_weight, random_state=SEED)\n",
    "        cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "        score = cross_val_score(model, X_train, Y_train, cv=cv, scoring='accuracy').mean()\n",
    "        scores.append(score)\n",
    "    \n",
    "    return 1 - np.mean(scores)\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "study.optimize(svm_objective, n_trials=50)\n",
    "\n",
    "\n",
    "print(\"Best Hyperparameters:\", study.best_params)\n",
    "final_model = SVC(**study.best_params, random_state=SEED, probability=True)\n",
    "run_model_by_conferences(final_model, 11)\n",
    "BEST_MODELS[\"svm\"] = run_model_by_conferences(final_model, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def knn_objective(trial):\n",
    "#     # Define the hyperparameter search space for KNN Classifier\n",
    "#     n_neighbors = trial.suggest_int('n_neighbors', 1, 30)\n",
    "#     weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "#     metric = trial.suggest_categorical('metric', ['minkowski', 'euclidean', 'manhattan', 'chebyshev'])\n",
    "#     p = trial.suggest_int('p', 1, 2)  # p for Minkowski distance\n",
    "#     algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "#     leaf_size = trial.suggest_int('leaf_size', 10, 40)\n",
    "#     n_jobs = trial.suggest_categorical('n_jobs', [-1])  # Use all processors\n",
    "\n",
    "#     X_train, Y_train, _, _ = get_train_and_test_data(teams, 10)\n",
    "\n",
    "#     model = KNeighborsClassifier(\n",
    "#         n_neighbors=n_neighbors,\n",
    "#         weights=weights,\n",
    "#         metric=metric,\n",
    "#         p=p,\n",
    "#         algorithm=algorithm,\n",
    "#         leaf_size=leaf_size,\n",
    "#         n_jobs=n_jobs\n",
    "#     )\n",
    "\n",
    "#     score = cross_val_score(model, X_train, Y_train, cv=3, scoring='accuracy').mean()\n",
    "\n",
    "#     return score\n",
    "\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "# study.optimize(knn_objective, n_trials=50)\n",
    "\n",
    "# print(\"Best Hyperparameters:\", study.best_params)\n",
    "\n",
    "# run_model_by_conferences(KNeighborsClassifier(**study.best_params), 10)\n",
    "# run_model_by_conferences(KNeighborsClassifier(**BEST_HYPER_PARAMETERS[\"knn\"]), 10) # best hyper parameters\n",
    "#\n",
    "# # predict test data with best hyper parameters with best hyper parameters\n",
    "# run_model_by_conferences(KNeighborsClassifier(**BEST_HYPER_PARAMETERS[\"knn\"]), 11) # best hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def naive_bayes_objective(trial):\n",
    "#     var_smoothing = trial.suggest_loguniform('var_smoothing', 1e-9, 1e-2)\n",
    "#     \n",
    "#     scores = []\n",
    "#     for i in range(2,11):\n",
    "#         X_train, Y_train, _, _ = get_train_and_test_data(teams, i)\n",
    "#         model = GaussianNB(var_smoothing=var_smoothing)\n",
    "#         cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "#         score = cross_val_score(model, X_train, Y_train, cv=cv, scoring='accuracy').mean()\n",
    "#         scores.append(score)\n",
    "#     \n",
    "#     return 1 - np.mean(scores)\n",
    "# \n",
    "# \n",
    "# study = optuna.create_study(direction='minimize',sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "# study.optimize(naive_bayes_objective, n_trials=50)\n",
    "# \n",
    "# print(\"Best Hyperparameters:\", study.best_params)\n",
    "# final_model = GaussianNB(**study.best_params)\n",
    "# run_model_by_conferences(final_model, 11)\n",
    "# BEST_MODELS['naive_bayes'] = run_model_by_conferences(final_model, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_forest_objective(trial):\n",
    "#     n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
    "#     max_depth = trial.suggest_int('max_depth', 1, 20)\n",
    "#     min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "#     min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
    "#     max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])  # Removed 'auto'\n",
    "#     bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "#     criterion = trial.suggest_categorical('criterion', ['squared_error', 'absolute_error'])\n",
    "# \n",
    "#     scores = []\n",
    "#     for i in range(2,11):\n",
    "#         X_train, Y_train, _, _ = get_train_and_test_data(teams, i)\n",
    "#         model = RandomForestRegressor(\n",
    "#             n_estimators=n_estimators,\n",
    "#             max_depth=max_depth,\n",
    "#             min_samples_split=min_samples_split,\n",
    "#             min_samples_leaf=min_samples_leaf,\n",
    "#             max_features=max_features,\n",
    "#             bootstrap=bootstrap,\n",
    "#             criterion=criterion,\n",
    "#             random_state=42\n",
    "#         )\n",
    "#         cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "#         score = cross_val_score(model, X_train, Y_train, cv=cv, scoring='neg_mean_squared_error').mean()\n",
    "#         scores.append(score)\n",
    "#     \n",
    "#     return 1 - np.mean(scores)\n",
    "#     \n",
    "#     \n",
    "# study = optuna.create_study(direction='maximize',sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "# study.optimize(random_forest_objective, n_trials=50)\n",
    "# \n",
    "# \n",
    "# print(\"Best Hyperparameters:\", study.best_params)\n",
    "# final_model = RandomForestRegressor(**study.best_params, random_state=42)\n",
    "# run_model_by_conferences(final_model, 11)\n",
    "# BEST_MODELS['random_forest'] = run_model_by_conferences(final_model, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_model_by_conferences(LinearRegression(), 10)\n",
    "# \n",
    "# # predict test data with best hyper parameters\n",
    "# run_model_by_conferences(LinearRegression(), 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_statistic_figure(statistic_name, statistic_results):\n",
    "#     labels = list(statistic_results.keys())\n",
    "#     values = list(statistic_results.values())\n",
    "# \n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     plt.barh(labels, values, color='skyblue', edgecolor='black')\n",
    "#     plt.title(\"Accuracy by Models\", fontsize=16)\n",
    "#     plt.xlabel(\"Accuracy\", fontsize=14)\n",
    "#     plt.ylabel(\"Models\", fontsize=14)\n",
    "#     plt.xticks(fontsize=12)\n",
    "#     plt.yticks(fontsize=12)\n",
    "#     plt.show()\n",
    "# \n",
    "# \n",
    "# results_accuracy = dict(sorted(results_accuracy.items(), key=lambda item: item[1]))\n",
    "# results_precision = dict(sorted(results_precision.items(), key=lambda item: item[1]))\n",
    "# results_error = dict(sorted(results_error.items(), key=lambda item: item[1]))\n",
    "# \n",
    "# show_statistic_figure(\"Accuracy\", results_accuracy)\n",
    "# show_statistic_figure(\"Precision\", results_precision)\n",
    "# show_statistic_figure(\"Error\", results_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data_prediction', exist_ok=True)\n",
    "\n",
    "for k,v in BEST_MODELS.items():\n",
    "    v.to_csv(os.path.join('data_prediction', k+'.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
